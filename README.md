# ðŸ“š flexiable-prefill-attention
ðŸ“š Flexible Flash Prefill Attention with O(1) SRAM complexity for large head dim (D > 256), almost 1.2x-1.5x faster than SDPA (EA) with MMA acc F32/F16. (Write for Fun ðŸ‘€~)
