# 📚 Flexible Flash Prefill Attention
⚡️ Flexible Flash Prefill Attention with **O(1) SRAM complexity** & **O(d/4) register complexity** for large head dim (D > 256), almost **1.2x-1.5x** 🎉 faster than SDPA (EA). (Write for Fun 👀~)
